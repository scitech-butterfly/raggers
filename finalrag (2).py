# -*- coding: utf-8 -*-
"""finalrag.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CEg_G3u2hkaQjKRGaCcTuS70W2w1Jf3g

# Installations
"""

"""# Imports"""

import os
import faiss
import json
import numpy as np
from typing import List, Dict
from functools import lru_cache
from sentence_transformers import SentenceTransformer
from langchain.text_splitter import RecursiveCharacterTextSplitter
import pdfplumber
import docx
from email import policy
from email.parser import BytesParser
from groq import Groq
from langchain_core.prompts import PromptTemplate

"""# Main Code"""

# --- LAZY LOADING: Models will be loaded on first use, not at startup ---
EMBEDDING_MODEL = None

def get_embedding_model():
    global EMBEDDING_MODEL
    if EMBEDDING_MODEL is None:
        print("Loading embedding model on first request...")
        EMBEDDING_MODEL = SentenceTransformer('sentence-transformers/paraphrase-albert-small-v2')
        print("Embedding model loaded successfully.")
    return EMBEDDING_MODEL


# 1. Document Loading & Chunking
def extract_text_from_pdf(path: str) -> List[Dict]:
    """Extracts text from each page of a PDF."""
    chunks = []
    with pdfplumber.open(path) as pdf:
        for i, page in enumerate(pdf.pages):
            text = page.extract_text()
            if text:
                chunks.append({"text": text.strip(), "page": i + 1})
    return chunks

def extract_text_from_docx(path: str) -> List[Dict]:
    """Extracts text from a DOCX document."""
    doc = docx.Document(path)
    full_text = "\n".join([para.text for para in doc.paragraphs if para.text.strip()])
    return [{"text": full_text, "page": 1}]

def extract_text_from_email(path: str) -> List[Dict]:
    """Extracts the body from an email file."""
    with open(path, 'rb') as f:
        msg = BytesParser(policy=policy.default).parse(f)
    body = msg.get_body(preferencelist=('plain')).get_content()
    return [{"text": body.strip(), "page": 1}]

def load_and_chunk_document(path: str, chunk_size=1000, chunk_overlap=200) -> List[Dict]:
    """
    Loads a document from the given path, extracts the text, and then chunks it
    using a RecursiveCharacterTextSplitter.
    """
    ext = os.path.splitext(path)[1].lower()
    if ext == ".pdf":
        raw_pages = extract_text_from_pdf(path)
    elif ext == ".docx":
        raw_pages = extract_text_from_docx(path)
    elif ext in [".eml", ".email"]:
        raw_pages = extract_text_from_email(path)
    else:
        raise ValueError("Unsupported file format")

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=chunk_size,
        chunk_overlap=chunk_overlap
    )

    all_chunks = []
    for page_info in raw_pages:
        page_text = page_info["text"]
        page_number = page_info["page"]

        recursive_chunks = text_splitter.split_text(page_text)

        for chunk_content in recursive_chunks:
            all_chunks.append({
                "content": chunk_content,
                "page": page_number,
                "source_file": os.path.basename(path)
            })

    return all_chunks

@lru_cache(maxsize=128)
def get_index_and_chunks(doc_path):
    """
    Caches the loaded chunks and FAISS index for a specific document path.
    This prevents re-loading and re-indexing the same document for every request.
    """
    chunks = load_and_chunk_document(doc_path)
    index = build_faiss_index(chunks)
    return chunks, index

# 2. Embedding + FAISS Indexing
def build_faiss_index(chunks: List[Dict]):
    """
    Builds a FAISS index from the provided chunks using the pre-loaded embedding model.
    """
    model = get_embedding_model()
    texts = [c['content'] for c in chunks]
    embeddings = model.encode(texts)

    dim = embeddings.shape[1]
    index = faiss.IndexFlatL2(dim)
    index.add(np.array(embeddings).astype('float32'))

    return index

# 3. Hybrid Semantic Retrieval
def search_top_chunks(query: str, chunks: List[Dict], index, k=5) -> List[Dict]:
    """
    Searches for the top k most relevant chunks using the pre-loaded index and model.
    """
    model = get_embedding_model()
    query_vec = model.encode([query])
    D, I = index.search(np.array(query_vec).astype('float32'), k)
    return [chunks[i] for i in I[0]]

def rerank_chunks(query: str, chunks: List[Dict], top_n=3) -> List[Dict]:
    """
    Re-ranks a list of chunks based on their relevance to the query using the CrossEncoder model.
    """
    # This function is removed to further reduce memory footprint
    return chunks[:top_n]


# 4. Prompt Template
prompt_template = PromptTemplate(
    input_variables=["query", "context"],
    template="""
You are an intelligent insurance assistant.

User has asked:
{query}

Refer to the following policy clauses to answer:
---
{context}
---

Important instructions:
- Do NOT assume any medical history or user behavior unless explicitly mentioned in the query.
- Do NOT confuse unrelated medical domains.
- Only use clauses that match the medical situation in the userâ€™s query.
- If a clause is not relevant, do not use it.

Your task:
- Return a decision: Approved / Rejected
- Estimated payout amount (if applicable)
- Justify based on which clause(s)
- Format the result in strict JSON with keys: "decision", "amount", "justification", "clause_reference"
"""
)

# 5. Call the LLM (Groq)
def run_inference(prompt: str, groq_client: Groq) -> Dict:
    try:
        chat_completion = groq_client.chat.completions.create(
            messages=[
                {
                    "role": "user",
                    "content": prompt,
                }
            ],
            model="llama3-8b-8192",
        )
        result = chat_completion.choices[0].message.content.strip()
        json_match = result[result.find('{'): result.rfind('}')+1]
        return json.loads(json_match)
    except json.JSONDecodeError as e:
        print(f"JSON parsing error: {e}")
        return {"error": "Could not parse output", "raw": result}
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
        return {"error": "An unexpected error occurred", "raw": str(e)}

# 6. End-to-End Runner (Enhanced)
def process_query(query: str, doc_path: str, groq_client: Groq):
    chunks, index = get_index_and_chunks(doc_path)
    initial_chunks = search_top_chunks(query, chunks, index, k=5)
    # Reranking is removed to save memory.
    
    context = "\n\n".join([f"[Clause - Pg {c['page']}] {c['content']}" for c in initial_chunks])
    prompt = prompt_template.format(query=query, context=context)
    return run_inference(prompt, groq_client)

groq_client = Groq(api_key="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")
